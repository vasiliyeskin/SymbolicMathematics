{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "beam_integration.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasiliyeskin/SymbolicMathematics/blob/master/beam_integration_for_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EQ3bCmmRGXS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc3f5949-779d-4947-d305-1b62a611d21b"
      },
      "source": [
        "!git clone https://github.com/vasiliyeskin/SymbolicMathematics"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'SymbolicMathematics'...\n",
            "remote: Enumerating objects: 34, done.\u001b[K\n",
            "remote: Counting objects: 100% (34/34), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 34 (delta 6), reused 27 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (34/34), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQOPy-JJRYlf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b50a29fd-765f-4cc8-e466-e6b2eca07cdc"
      },
      "source": [
        "%cd SymbolicMathematics"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/SymbolicMathematics\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0r_rmRAQ0df"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import sympy as sp\n",
        "import torch\n",
        "\n",
        "from src.utils import AttrDict\n",
        "from src.envs import build_env\n",
        "from src.model import build_modules\n",
        "\n",
        "from src.utils import to_cuda\n",
        "from src.envs.sympy_utils import simplify"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iM87Dle9Q0dm"
      },
      "source": [
        "## Build environment / Reload model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvjmMy1JQ0dm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "279dd204-1dad-4ab7-f1f7-7281afbd1f57"
      },
      "source": [
        "# trained model, e.g. \"wget https://dl.fbaipublicfiles.com/SymbolicMathematics/models/fwd_bwd.pth\"\n",
        "!wget https://dl.fbaipublicfiles.com/SymbolicMathematics/models/fwd_bwd.pth\n",
        "model_path = './fwd_bwd.pth'\n",
        "assert os.path.isfile(model_path)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-26 21:11:18--  https://dl.fbaipublicfiles.com/SymbolicMathematics/models/fwd_bwd.pth\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 172.67.9.4, 104.22.74.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1849506143 (1.7G) [application/octet-stream]\n",
            "Saving to: ‘fwd_bwd.pth’\n",
            "\n",
            "fwd_bwd.pth         100%[===================>]   1.72G  27.5MB/s    in 65s     \n",
            "\n",
            "2020-12-26 21:12:24 (27.1 MB/s) - ‘fwd_bwd.pth’ saved [1849506143/1849506143]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcoo8vj2Q0dn"
      },
      "source": [
        "params = params = AttrDict({\n",
        "\n",
        "    # environment parameters\n",
        "    'env_name': 'char_sp',\n",
        "    'int_base': 10,\n",
        "    'balanced': False,\n",
        "    'positive': True,\n",
        "    'precision': 10,\n",
        "    'n_variables': 1,\n",
        "    'n_coefficients': 0,\n",
        "    'leaf_probs': '0.75,0,0.25,0',\n",
        "    'max_len': 512,\n",
        "    'max_int': 5,\n",
        "    'max_ops': 15,\n",
        "    'max_ops_G': 15,\n",
        "    'clean_prefix_expr': True,\n",
        "    'rewrite_functions': '',\n",
        "    'tasks': 'prim_fwd',\n",
        "    'operators': 'add:10,sub:3,mul:10,div:5,sqrt:4,pow2:4,pow3:2,pow4:1,pow5:1,ln:4,exp:4,sin:4,cos:4,tan:4,asin:1,acos:1,atan:1,sinh:1,cosh:1,tanh:1,asinh:1,acosh:1,atanh:1',\n",
        "\n",
        "    # model parameters\n",
        "    'cpu': False,\n",
        "    'emb_dim': 1024,\n",
        "    'n_enc_layers': 6,\n",
        "    'n_dec_layers': 6,\n",
        "    'n_heads': 8,\n",
        "    'dropout': 0,\n",
        "    'attention_dropout': 0,\n",
        "    'sinusoidal_embeddings': False,\n",
        "    'share_inout_emb': True,\n",
        "    'reload_model': model_path,\n",
        "\n",
        "})"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PX7-Pabh6dfy"
      },
      "source": [
        "def count_parameters(model):\r\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qShga6qLQ0dn"
      },
      "source": [
        "env = build_env(params)\n",
        "x = env.local_dict['x']"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDsbJQpcQ0dn",
        "outputId": "90f9a37f-7132-4adb-e37a-c5de8e44180d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "modules = build_modules(env, params)\n",
        "encoder = modules['encoder']\n",
        "decoder = modules['decoder']\n",
        "\n",
        "\n",
        "print(encoder)\n",
        "print(f'The model has {count_parameters(encoder):,} trainable parameters')\n",
        "\n",
        "print(decoder)\n",
        "print(f'The model has {count_parameters(decoder):,} trainable parameters')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TransformerModel(\n",
            "  (position_embeddings): Embedding(4096, 1024)\n",
            "  (embeddings): Embedding(91, 1024, padding_idx=1)\n",
            "  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "  (attentions): ModuleList(\n",
            "    (0): MultiHeadAttention(\n",
            "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    )\n",
            "    (1): MultiHeadAttention(\n",
            "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    )\n",
            "    (2): MultiHeadAttention(\n",
            "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    )\n",
            "    (3): MultiHeadAttention(\n",
            "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    )\n",
            "    (4): MultiHeadAttention(\n",
            "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    )\n",
            "    (5): MultiHeadAttention(\n",
            "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (layer_norm1): ModuleList(\n",
            "    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "  )\n",
            "  (ffns): ModuleList(\n",
            "    (0): TransformerFFN(\n",
            "      (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "      (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "    )\n",
            "    (1): TransformerFFN(\n",
            "      (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "      (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "    )\n",
            "    (2): TransformerFFN(\n",
            "      (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "      (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "    )\n",
            "    (3): TransformerFFN(\n",
            "      (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "      (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "    )\n",
            "    (4): TransformerFFN(\n",
            "      (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "      (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "    )\n",
            "    (5): TransformerFFN(\n",
            "      (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "      (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (layer_norm2): ModuleList(\n",
            "    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "  )\n",
            ")\n",
            "The model has 79,866,880 trainable parameters\n",
            "TransformerModel(\n",
            "  (position_embeddings): Embedding(4096, 1024)\n",
            "  (embeddings): Embedding(91, 1024, padding_idx=1)\n",
            "  (layer_norm_emb): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "  (attentions): ModuleList(\n",
            "    (0): MultiHeadAttention(\n",
            "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    )\n",
            "    (1): MultiHeadAttention(\n",
            "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    )\n",
            "    (2): MultiHeadAttention(\n",
            "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    )\n",
            "    (3): MultiHeadAttention(\n",
            "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    )\n",
            "    (4): MultiHeadAttention(\n",
            "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    )\n",
            "    (5): MultiHeadAttention(\n",
            "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (layer_norm1): ModuleList(\n",
            "    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "  )\n",
            "  (ffns): ModuleList(\n",
            "    (0): TransformerFFN(\n",
            "      (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "      (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "    )\n",
            "    (1): TransformerFFN(\n",
            "      (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "      (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "    )\n",
            "    (2): TransformerFFN(\n",
            "      (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "      (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "    )\n",
            "    (3): TransformerFFN(\n",
            "      (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "      (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "    )\n",
            "    (4): TransformerFFN(\n",
            "      (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "      (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "    )\n",
            "    (5): TransformerFFN(\n",
            "      (lin1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "      (lin2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (layer_norm2): ModuleList(\n",
            "    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "  )\n",
            "  (layer_norm15): ModuleList(\n",
            "    (0): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "    (1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "    (2): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "    (3): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "    (4): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "    (5): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "  )\n",
            "  (encoder_attn): ModuleList(\n",
            "    (0): MultiHeadAttention(\n",
            "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    )\n",
            "    (1): MultiHeadAttention(\n",
            "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    )\n",
            "    (2): MultiHeadAttention(\n",
            "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    )\n",
            "    (3): MultiHeadAttention(\n",
            "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    )\n",
            "    (4): MultiHeadAttention(\n",
            "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    )\n",
            "    (5): MultiHeadAttention(\n",
            "      (q_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (k_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (v_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (out_lin): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (proj): Linear(in_features=1024, out_features=91, bias=True)\n",
            ")\n",
            "The model has 105,069,659 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEUp9GW1Q0do"
      },
      "source": [
        "## Start from a function F, compute its derivative f = F', and try to recover F from f"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrWO74riQ0do"
      },
      "source": [
        "# here you can modify the integral function the model has to predict, F\n",
        "F_infix = 'x * tan(exp(x)/x)'\n",
        "F_infix = 'x * cos(x**2) * tan(x)'\n",
        "F_infix = 'cos(x**2 * exp(x * cos(x)))'\n",
        "F_infix = 'ln(cos(x + exp(x)) * sin(x**2 + 2) * exp(x) / x)'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIn3Gk8WQ0do",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0fb75c8-f5e4-4eae-82ec-74b0af4ea0ae"
      },
      "source": [
        "# F (integral, that the model will try to predict)\n",
        "F = sp.S(F_infix, locals=env.local_dict)\n",
        "F"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "log(exp(x)*sin(x**2 + 2)*cos(x + exp(x))/x)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCSi1V4yQ0dp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a6c45c6-0901-46f8-95ab-9d2651d68f2b"
      },
      "source": [
        "# f (F', that the model will take as input)\n",
        "f = F.diff(x)\n",
        "f"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "x*(2*exp(x)*cos(x + exp(x))*cos(x**2 + 2) - (exp(x) + 1)*exp(x)*sin(x + exp(x))*sin(x**2 + 2)/x + exp(x)*sin(x**2 + 2)*cos(x + exp(x))/x - exp(x)*sin(x**2 + 2)*cos(x + exp(x))/x**2)*exp(-x)/(sin(x**2 + 2)*cos(x + exp(x)))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BFijeWoQ0dq"
      },
      "source": [
        "### Compute prefix representations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "rY9qSXurQ0dq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abd03ad0-bbc3-4961-e356-fd5e838a987c"
      },
      "source": [
        "F_prefix = env.sympy_to_prefix(F)\n",
        "f_prefix = env.sympy_to_prefix(f)\n",
        "print(f\"F prefix: {F_prefix}\")\n",
        "print(f\"f prefix: {f_prefix}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F prefix: ['ln', 'mul', 'pow', 'x', 'INT-', '1', 'mul', 'cos', 'add', 'x', 'exp', 'x', 'mul', 'exp', 'x', 'sin', 'add', 'INT+', '2', 'pow', 'x', 'INT+', '2']\n",
            "f prefix: ['mul', 'x', 'mul', 'pow', 'cos', 'add', 'x', 'exp', 'x', 'INT-', '1', 'mul', 'pow', 'sin', 'add', 'INT+', '2', 'pow', 'x', 'INT+', '2', 'INT-', '1', 'mul', 'add', 'mul', 'INT+', '2', 'mul', 'cos', 'add', 'INT+', '2', 'pow', 'x', 'INT+', '2', 'mul', 'cos', 'add', 'x', 'exp', 'x', 'exp', 'x', 'add', 'mul', 'pow', 'x', 'INT-', '1', 'mul', 'cos', 'add', 'x', 'exp', 'x', 'mul', 'exp', 'x', 'sin', 'add', 'INT+', '2', 'pow', 'x', 'INT+', '2', 'add', 'mul', 'INT-', '1', 'mul', 'pow', 'x', 'INT-', '2', 'mul', 'cos', 'add', 'x', 'exp', 'x', 'mul', 'exp', 'x', 'sin', 'add', 'INT+', '2', 'pow', 'x', 'INT+', '2', 'mul', 'INT-', '1', 'mul', 'pow', 'x', 'INT-', '1', 'mul', 'add', 'INT+', '1', 'exp', 'x', 'mul', 'exp', 'x', 'mul', 'sin', 'add', 'INT+', '2', 'pow', 'x', 'INT+', '2', 'sin', 'add', 'x', 'exp', 'x', 'exp', 'mul', 'INT-', '1', 'x']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wp05z6uhQ0dq"
      },
      "source": [
        "### Encode input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmCtmk78Q0dr"
      },
      "source": [
        "x1_prefix = env.clean_prefix(['sub', 'derivative', 'f', 'x', 'x'] + f_prefix)\n",
        "x1 = torch.LongTensor(\n",
        "    [env.eos_index] +\n",
        "    [env.word2id[w] for w in x1_prefix] +\n",
        "    [env.eos_index]\n",
        ").view(-1, 1)\n",
        "len1 = torch.LongTensor([len(x1)])\n",
        "x1, len1 = to_cuda(x1, len1)\n",
        "\n",
        "with torch.no_grad():\n",
        "    encoded = encoder('fwd', x=x1, lengths=len1, causal=False).transpose(0, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUR1WhF6Q0dr"
      },
      "source": [
        "### Decode with beam search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGa5rBgRQ0dr"
      },
      "source": [
        "beam_size = 10\n",
        "with torch.no_grad():\n",
        "    _, _, beam = decoder.generate_beam(encoded, len1, beam_size=beam_size, length_penalty=1.0, early_stopping=1, max_len=200)\n",
        "    assert len(beam) == 1\n",
        "hypotheses = beam[0].hyp\n",
        "assert len(hypotheses) == beam_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR4bActTQ0dr"
      },
      "source": [
        "### Print results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "2_nNLms2Q0ds",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3846b3ae-16b1-4723-d0c9-fae4359cf8c4"
      },
      "source": [
        "print(f\"Input function f: {f}\")\n",
        "print(f\"Reference function F: {F}\")\n",
        "print(\"\")\n",
        "\n",
        "for score, sent in sorted(hypotheses, key=lambda x: x[0], reverse=True):\n",
        "\n",
        "    # parse decoded hypothesis\n",
        "    ids = sent[1:].tolist()                  # decoded token IDs\n",
        "    tok = [env.id2word[wid] for wid in ids]  # convert to prefix\n",
        "\n",
        "    try:\n",
        "        hyp = env.prefix_to_infix(tok)       # convert to infix\n",
        "        hyp = env.infix_to_sympy(hyp)        # convert to SymPy\n",
        "\n",
        "        # check whether we recover f if we differentiate the hypothesis\n",
        "        # note that sometimes, SymPy fails to show that hyp' - f == 0, and the result is considered as invalid, although it may be correct\n",
        "        res = \"OK\" if simplify(hyp.diff(x) - f, seconds=1) == 0 else \"NO\"\n",
        "\n",
        "    except:\n",
        "        res = \"INVALID PREFIX EXPRESSION\"\n",
        "        hyp = tok\n",
        "\n",
        "    # print result\n",
        "    print(\"%.5f  %s  %s\" % (score, res, hyp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input function f: x*(2*exp(x)*cos(x + exp(x))*cos(x**2 + 2) - (exp(x) + 1)*exp(x)*sin(x + exp(x))*sin(x**2 + 2)/x + exp(x)*sin(x**2 + 2)*cos(x + exp(x))/x - exp(x)*sin(x**2 + 2)*cos(x + exp(x))/x**2)*exp(-x)/(sin(x**2 + 2)*cos(x + exp(x)))\n",
            "Reference function F: log(exp(x)*sin(x**2 + 2)*cos(x + exp(x))/x)\n",
            "\n",
            "-0.00003  OK  log(exp(x)*sin(x**2 + 2)*cos(x + exp(x))/x)\n",
            "-0.28475  OK  log(exp(x)*sin((x**3 + 2*x)/x)*cos(x + exp(x))/x)\n",
            "-0.28592  OK  log(exp(x)*sin(x*(x + 2/x))*cos(x + exp(x))/x)\n",
            "-0.35794  OK  log(exp(x)*sin(x*(x + 1) - x + 2)*cos(x + exp(x))/x)\n",
            "-0.37952  NO  log(exp(x)*sin(x**2*(x + 2/x))*cos(x + exp(x))/x)\n",
            "-0.38034  NO  log(exp(x)*sin(x**2 + 2)*cos(x + sinh(x) + cosh(x))/x)\n",
            "-0.39518  OK  atan(tan(log(exp(x)*sin(x**2 + 2)*cos(x + exp(x))/x)))\n",
            "-0.39689  OK  log(exp(x)*sin(x*(x - 1) + x + 2)*cos(x + exp(x))/x)\n",
            "-0.43203  NO  log(exp(x)*sin((x**2 + 2)**2)*cos(x + exp(x))/x)\n",
            "-0.44538  NO  log(exp(x)*sin(x**2 + 2*x)*cos(x + exp(x))/x)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}